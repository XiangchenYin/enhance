{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43f39fff-e583-437a-a41e-c85496408b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CRU(nn.Module):\n",
    "    '''\n",
    "    alpha: 0<alpha<1\n",
    "    '''\n",
    "    def __init__(self, \n",
    "                 op_channel:int,\n",
    "                 alpha:float = 1/2,\n",
    "                 squeeze_radio:int = 2 ,\n",
    "                 group_size:int = 2,\n",
    "                 group_kernel_size:int = 3,\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.up_channel     = up_channel   =   int(alpha*op_channel)\n",
    "        self.low_channel    = low_channel  =   op_channel-up_channel\n",
    "        self.squeeze1       = nn.Conv2d(up_channel,up_channel//squeeze_radio,kernel_size=1,bias=False)\n",
    "        self.squeeze2       = nn.Conv2d(low_channel,low_channel//squeeze_radio,kernel_size=1,bias=False)\n",
    "        #up\n",
    "        self.GWC            = nn.Conv2d(up_channel//squeeze_radio, op_channel,kernel_size=group_kernel_size, stride=1,padding=group_kernel_size//2, groups = group_size)\n",
    "        self.PWC1           = nn.Conv2d(up_channel//squeeze_radio, op_channel,kernel_size=1, bias=False)\n",
    "        #low\n",
    "        self.PWC2           = nn.Conv2d(low_channel//squeeze_radio, op_channel-low_channel//squeeze_radio,kernel_size=1, bias=False)\n",
    "        self.advavg         = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "    def forward(self,x):\n",
    "        # Split\n",
    "        up,low  = torch.split(x,[self.up_channel,self.low_channel],dim=1)\n",
    "        up,low  = self.squeeze1(up),self.squeeze2(low)\n",
    "        # Transform\n",
    "        Y1      = self.GWC(up) + self.PWC1(up)\n",
    "        Y2      = torch.cat( [self.PWC2(low), low], dim= 1 )\n",
    "        # Fuse\n",
    "        out     = torch.cat( [Y1, Y2], dim= 1 )\n",
    "        out     = F.softmax( self.advavg(out), dim=1 ) * out\n",
    "        out1,out2 = torch.split(out,out.size(1)//2,dim=1)\n",
    "        return out1+out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1fba29d4-2712-457f-a319-1686b86207eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "img = torch.ones(1,16,64,64)\n",
    "\n",
    "\n",
    "oc = CRU(op_channel = 16)\n",
    "\n",
    "print(oc(img).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e8094253-249e-4fad-bf9b-c7b8ad839a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, ffn_expansion_factor, bias):\n",
    "        super(FeedForward, self).__init__()\n",
    "\n",
    "        hidden_features = int(dim*ffn_expansion_factor)\n",
    "\n",
    "        self.project_in = nn.Conv2d(dim, hidden_features*2, kernel_size=1, bias=bias)\n",
    "\n",
    "        self.dwconv = nn.Conv2d(hidden_features*2, hidden_features*2, kernel_size=3, stride=1, padding=1, groups=hidden_features*2, bias=bias)\n",
    "\n",
    "        self.project_out = nn.Conv2d(hidden_features, dim, kernel_size=1, bias=bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.project_in(x)\n",
    "        print(self.dwconv(x).shape)\n",
    "        x1, x2 = self.dwconv(x).chunk(2, dim=1)\n",
    "        print(x1.shape, x2.shape)\n",
    "        x = F.gelu(x2)*x1 + F.gelu(x1)*x2\n",
    "        x = self.project_out(x)\n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f115f7df-efc3-47b5-8362-b4dcc2e0130f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 84, 64, 64])\n",
      "torch.Size([1, 42, 64, 64]) torch.Size([1, 42, 64, 64])\n",
      "torch.Size([1, 16, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "ffn = FeedForward(dim=16, ffn_expansion_factor=2.66, bias=False)\n",
    "img = torch.ones(1,16,64,64)\n",
    "\n",
    "print(ffn(img).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "acf87859-45a7-48be-af2e-ffd338a0b3b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x torch.Size([8, 3, 400, 600])\n",
      "conv_large torch.Size([8, 64, 100, 150])\n",
      "before attention torch.Size([8, 15000, 64])\n",
      "qqqq torch.Size([8, 15000, 64])\n",
      "after attention torch.Size([8, 10, 64])\n",
      "xxx torch.Size([8, 10, 64])\n",
      "generator torch.Size([8, 10, 64])\n",
      "gamma, color torch.Size([8, 1, 64]) torch.Size([8, 9, 64])\n",
      "torch.Size([8, 1]) torch.Size([8, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "import imp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from timm.models.layers import trunc_normal_, DropPath, to_2tuple\n",
    "import os\n",
    "from model.blocks import Mlp\n",
    "\n",
    "\n",
    "class query_Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=2, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        # NOTE scale factor was wrong in my original version, can set manually to be compat with prev weights\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "        self.q = nn.Parameter(torch.ones((1, 10, dim)), requires_grad=True)\n",
    "        self.k = nn.Linear(dim, dim, bias=qkv_bias)\n",
    "        self.v = nn.Linear(dim, dim, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print('before attention', x.shape)\n",
    "        B, N, C = x.shape\n",
    "        \n",
    "        k = self.k(x).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n",
    "        v = self.v(x).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n",
    "\n",
    "        q = self.q.expand(B, -1, -1).view(B, -1, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, 10, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        \n",
    "        print('after attention', x.shape)\n",
    "        return x\n",
    "\n",
    "\n",
    "class query_SABlock(nn.Module):\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.pos_embed = nn.Conv2d(dim, dim, 3, padding=1, groups=dim)\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = query_Attention(\n",
    "            dim,\n",
    "            num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "            attn_drop=attn_drop, proj_drop=drop)\n",
    "        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pos_embed(x)\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        x = self.drop_path(self.attn(self.norm1(x)))\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "\n",
    "class conv_embedding(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(conv_embedding, self).__init__()\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels // 2, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)),\n",
    "            nn.BatchNorm2d(out_channels // 2),\n",
    "            nn.GELU(),\n",
    "            # nn.Conv2d(out_channels // 2, out_channels // 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            # nn.BatchNorm2d(out_channels // 2),\n",
    "            # nn.GELU(),\n",
    "            nn.Conv2d(out_channels // 2, out_channels, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Global_pred(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=64, num_heads=4, type='exp'):\n",
    "        super(Global_pred, self).__init__()\n",
    "        if type == 'exp':\n",
    "            self.gamma_base = nn.Parameter(torch.ones((1)), requires_grad=False) # False in exposure correction\n",
    "        else:\n",
    "            self.gamma_base = nn.Parameter(torch.ones((1)), requires_grad=True)  \n",
    "        self.color_base = nn.Parameter(torch.eye((3)), requires_grad=True)  # basic color matrix\n",
    "        # main blocks\n",
    "        self.conv_large = conv_embedding(in_channels, out_channels)\n",
    "        self.generator = query_SABlock(dim=out_channels, num_heads=num_heads)\n",
    "        self.gamma_linear = nn.Linear(out_channels, 1)\n",
    "        self.color_linear = nn.Linear(out_channels, 1)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "        for name, p in self.named_parameters():\n",
    "            if name == 'generator.attn.v.weight':\n",
    "                nn.init.constant_(p, 0)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print('x', x.shape)\n",
    "        #print(self.gamma_base)\n",
    "        \n",
    "        x = self.conv_large(x)\n",
    "        print('conv_large', x.shape)\n",
    "        x = self.generator(x)\n",
    "        print('generator', x.shape)\n",
    "        gamma, color = x[:, 0].unsqueeze(1), x[:, 1:]\n",
    "        print('gamma, color', gamma.shape, color.shape)\n",
    "        gamma = self.gamma_linear(gamma).squeeze(-1) + self.gamma_base\n",
    "        #print(self.gamma_base, self.gamma_linear(gamma))\n",
    "        color = self.color_linear(color).squeeze(-1).view(-1, 3, 3) + self.color_base\n",
    "        return gamma, color\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    os.environ['CUDA_VISIBLE_DEVICES']='3'\n",
    "\n",
    "    img = torch.Tensor(8, 3, 400, 600)\n",
    "    global_net = Global_pred()\n",
    "    gamma, color = global_net(img)\n",
    "    print(gamma.shape, color.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ac803d-efe0-4a2d-a4ce-d0da9c87647d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PA(nn.Module):\n",
    "    \"\"\" Position attention module\"\"\"\n",
    "    #Ref from SAGAN\n",
    "    def __init__(self, in_dim):\n",
    "        super(PA, self).__init__()\n",
    "        self.chanel_in = in_dim\n",
    "\n",
    "        self.query_conv = nn.Conv2d(in_channels=in_dim, out_channels=in_dim//8, kernel_size=1)\n",
    "        self.key_conv = nn.Conv2d(in_channels=in_dim, out_channels=in_dim//8, kernel_size=1)\n",
    "        self.value_conv = nn.Conv2d(in_channels=in_dim, out_channels=in_dim, kernel_size=1)\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "    def forward(self, x, fft_map=None):\n",
    "        \"\"\"\n",
    "            inputs :\n",
    "                x : input feature maps( B X C X H X W)\n",
    "            returns :\n",
    "                out : attention value + input feature\n",
    "                attention: B X (HxW) X (HxW)\n",
    "        \"\"\"\n",
    "        fft_map = fft_map.float()\n",
    "        m_batchsize, C, height, width = x.size()\n",
    "        proj_query = self.query_conv(fft_map).view(m_batchsize, -1, width*height).permute(0, 2, 1)\n",
    "        proj_key = self.key_conv(x).view(m_batchsize, -1, width*height)\n",
    "        energy = torch.bmm(proj_query, proj_key)\n",
    "        attention = self.softmax(energy)\n",
    "        proj_value = self.value_conv(x).view(m_batchsize, -1, width*height)\n",
    "\n",
    "        out = torch.bmm(proj_value, attention.permute(0, 2, 1))\n",
    "        out = out.view(m_batchsize, C, height, width)\n",
    "\n",
    "        out = self.gamma*out + x\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cb8b9f1-f8c5-4bc0-b699-99cfd16f0607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 3, 400, 600])\n",
      "torch.Size([8, 32, 400, 600])\n",
      "torch.Size([8, 3, 400, 600])\n",
      "0.6689531803131104\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "class conv_embedding(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(conv_embedding, self).__init__()\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.GELU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)\n",
    "        return x\n",
    "    \n",
    "class CA(nn.Module):\n",
    "    \"\"\" Channel attention module\"\"\"\n",
    "    def __init__(self, in_dim):\n",
    "        super(CA, self).__init__()\n",
    "        self.chanel_in = in_dim\n",
    "\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "        self.softmax  = nn.Softmax(dim=-1)\n",
    "    def forward(self,x, fft_map=None):\n",
    "        \"\"\"\n",
    "            inputs :\n",
    "                x : input feature maps( B X C X H X W)\n",
    "            returns :\n",
    "                out : attention value + input feature\n",
    "                attention: B X C X C\n",
    "        \"\"\"\n",
    "        m_batchsize, C, height, width = x.size()\n",
    "        proj_query = x.view(m_batchsize, C, -1)\n",
    "        proj_key = x.view(m_batchsize, C, -1).permute(0, 2, 1)\n",
    "        energy = torch.bmm(proj_query, proj_key)\n",
    "        energy_new = torch.max(energy, -1, keepdim=True)[0].expand_as(energy)-energy\n",
    "        attention = self.softmax(energy_new)\n",
    "        proj_value = x.view(m_batchsize, C, -1)\n",
    "\n",
    "        out = torch.bmm(attention, proj_value)\n",
    "        out = out.view(m_batchsize, C, height, width)\n",
    "\n",
    "        out = self.gamma*out + x\n",
    "        return out\n",
    "    \n",
    "    \n",
    "\n",
    "def fft_img(img):\n",
    "    \n",
    "    f = np.fft.fft2(img)\n",
    "    fshift = np.fft.fftshift(f)\n",
    "    rows, cols = img.shape\n",
    "    crow, ccol = int(rows/2), int(cols/2)\n",
    "\n",
    "    fshift[crow-30:crow+30, ccol-30:ccol+30] = 0\n",
    "    ishift = np.fft.ifftshift(fshift)\n",
    "    iimg = np.fft.ifft2(ishift)\n",
    "    iimg = np.abs(iimg) \n",
    "    \n",
    "    return transforms.ToTensor()(iimg)\n",
    "    \n",
    "class FFT_Block(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=32, num_heads=4, type='exp'):\n",
    "        super(FFT_Block, self).__init__()\n",
    "        # main blocks\n",
    "        self.conv_large = conv_embedding(in_channels, out_channels)\n",
    "        self.generator = CA(in_dim=out_channels)\n",
    "        \n",
    "#         self.up = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        self.conv_fft = nn.Conv2d(3, 3, kernel_size=3, stride=(1, 1), padding=(1, 1))\n",
    "        self.conv_A = nn.Conv2d(out_channels, 3, kernel_size=3, stride=(1, 1), padding=(1, 1))\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        fft_x = self.conv_fft(x)\n",
    "        b,c,h,w = fft_x.shape\n",
    "        fft_map = torch.zeros(b, 32, h, w)\n",
    "        for i in range(b):\n",
    "            \n",
    "            img = fft_x[i, :, :, :].cpu().detach().numpy().transpose(1,2,0) # b c h w -> h w c\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            fft = fft_img(img*255)/255\n",
    "            fft = torch.cat([fft for i in range(32)], dim=0)\n",
    "            \n",
    "            fft_map[i, :,:,:] = fft\n",
    "        print(fft_map.shape)   \n",
    "        x = self.conv_large(x)\n",
    "        x = self.generator(x, fft_map)      \n",
    "        x = self.conv_A(x)\n",
    "        return x  \n",
    " \n",
    "\n",
    "import time\n",
    "fb = FFT_Block()\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "# img = cv2.imread('1.png')\n",
    "# img = cv2.resize(img, (600, 400))\n",
    "\n",
    "\n",
    "# img = transforms.ToTensor()(img).unsqueeze(0)\n",
    "img = torch.Tensor(8, 3, 400, 600)\n",
    "print(img.shape)\n",
    "print(fb(img).shape)\n",
    "end = time.time()\n",
    "\n",
    "\n",
    "# print(pa(img).shape)\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56368bf9-a9e9-4eae-883c-a0919bda01f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "496ee879-6eb7-4582-be0b-9570f4b6d20f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 400, 600])\n",
      "torch.Size([8, 4, 400, 600])\n"
     ]
    }
   ],
   "source": [
    " class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_planes, ratio=2):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "\n",
    "        self.fc1 = nn.Conv2d(in_planes, in_planes // ratio, 1, bias=False)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Conv2d(in_planes // ratio, in_planes, 1, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = self.fc2(self.relu1(self.fc1(self.avg_pool(x))))\n",
    "        max_out = self.fc2(self.relu1(self.fc1(self.max_pool(x))))\n",
    "        out = avg_out + max_out\n",
    "        return self.sigmoid(out)*x\n",
    "\n",
    "\n",
    "img = torch.Tensor(8, 4, 400, 600)\n",
    "print(img.shape)\n",
    "print(ChannelAttention(4)(img).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de8aa213-23ae-4730-872a-0bfe11b9eca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 400, 600])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_661/433983270.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m400\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m600\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_Attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_661/433983270.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 3)"
     ]
    }
   ],
   "source": [
    "class query_Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=2, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        # NOTE scale factor was wrong in my original version, can set manually to be compat with prev weights\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "        self.q = nn.Parameter(torch.ones((1, 10, dim)), requires_grad=True)\n",
    "        self.k = nn.Linear(dim, dim, bias=qkv_bias)\n",
    "        self.v = nn.Linear(dim, dim, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        k = self.k(x).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n",
    "        v = self.v(x).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n",
    "\n",
    "        q = self.q.expand(B, -1, -1).view(B, -1, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, 10, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "img = torch.Tensor(8, 4, 400, 600)\n",
    "print(img.shape)\n",
    "print(query_Attention(4)(img).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40f9e137-15a6-49df-bfd4-a7fdf670de61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 15000, 32])\n",
      "torch.Size([8, 15000, 1])\n"
     ]
    }
   ],
   "source": [
    "from model.blocks import CBlock_ln, SwinTransformerBlock\n",
    "\n",
    "gloabel = nn.Sequential(\n",
    "            nn.Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.BatchNorm2d(4),\n",
    "            nn.GELU(),\n",
    "        )\n",
    "linear = nn.Linear(32,1)\n",
    "img = torch.Tensor(8, 32, 100,150)\n",
    "img = img.reshape(img.shape[0], -1, img.shape[1])\n",
    "print(img.shape)\n",
    "print(linear(img).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d92b4f2-40d6-4f0e-9841-17a9dacdb9c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 32, 100, 150])\n",
      "torch.Size([8, 15000, 32])\n",
      "torch.Size([8, 32, 100, 150])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "class DPM(nn.Module):\n",
    "    def __init__(self, inplanes, planes, act=nn.LeakyReLU(negative_slope=0.2,inplace=True), bias=False):\n",
    "        super(DPM, self).__init__()\n",
    "\n",
    "        self.conv_mask = nn.Conv2d(inplanes, inplanes, kernel_size=1, bias=bias)\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        self.channel_add_conv = nn.Sequential(\n",
    "            nn.Conv2d(inplanes, planes, kernel_size=1, bias=bias),\n",
    "            act,\n",
    "            nn.Conv2d(planes, inplanes, kernel_size=1, bias=bias)\n",
    "        )\n",
    "        self.linear = nn.Linear(inplanes, 1)\n",
    "\n",
    "    def spatial_pool(self, x):\n",
    "        batch, channel, height, width = x.size()\n",
    "        input_x = x\n",
    "        input_x = input_x.view(batch, channel, height * width)\n",
    "        input_x = input_x.unsqueeze(1)\n",
    "        \n",
    "        context_mask = self.conv_mask(x)\n",
    "        context_mask = context_mask.view(batch, -1, channel)\n",
    "        print(context_mask.shape)\n",
    "        context_mask = self.linear(context_mask)\n",
    "        context_mask = context_mask.unsqueeze(1)\n",
    "        context = torch.matmul(input_x, context_mask)\n",
    "        context = context.view(batch, channel, 1, 1)\n",
    "\n",
    "        return context\n",
    "\n",
    "    def forward(self, x):\n",
    "        # [N, C, 1, 1]\n",
    "        context = self.spatial_pool(x)\n",
    "        channel_add_term = self.channel_add_conv(context)\n",
    "        x = x + channel_add_term\n",
    "        return x\n",
    "img = torch.Tensor(8, 32, 100,150)\n",
    "print(img.shape)\n",
    "print(DPM(32,32)(img).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a536d704-7a10-4507-80c6-e694227b9bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 32, 100, 150])\n",
      "torch.Size([8, 32, 100, 150])\n",
      "0.01157999038696289\n"
     ]
    }
   ],
   "source": [
    "from model.drconv import DRConv2d\n",
    "import functools\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules.conv import _ConvNd\n",
    "from torch.nn.modules.utils import _pair\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "\n",
    "class _routing(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, num_experts, dropout_rate):\n",
    "        super(_routing, self).__init__()\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc = nn.Linear(in_channels, num_experts)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        return F.sigmoid(x)\n",
    "    \n",
    "\n",
    "class CondConv2D(_ConvNd):\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1,\n",
    "                 padding=0, dilation=1, groups=1,\n",
    "                 bias=True, padding_mode='zeros', num_experts=3, dropout_rate=0.2):\n",
    "        kernel_size = _pair(kernel_size)\n",
    "        stride = _pair(stride)\n",
    "        padding = _pair(padding)\n",
    "        dilation = _pair(dilation)\n",
    "        super(CondConv2D, self).__init__(\n",
    "            in_channels, out_channels, kernel_size, stride, padding, dilation,\n",
    "            False, _pair(0), groups, bias, padding_mode)\n",
    "\n",
    "        self._avg_pooling = functools.partial(F.adaptive_avg_pool2d, output_size=(1, 1))\n",
    "        self._routing_fn = _routing(in_channels, num_experts, dropout_rate)\n",
    "        \n",
    "        self.weight = Parameter(torch.Tensor(\n",
    "            num_experts, out_channels, in_channels // groups, *kernel_size))\n",
    "        \n",
    "        self.reset_parameters()\n",
    "\n",
    "    def _conv_forward(self, input, weight):\n",
    "        if self.padding_mode != 'zeros':\n",
    "            return F.conv2d(F.pad(input, self._padding_repeated_twice, mode=self.padding_mode),\n",
    "                            weight, self.bias, self.stride,\n",
    "                            _pair(0), self.dilation, self.groups)\n",
    "        return F.conv2d(input, weight, self.bias, self.stride,\n",
    "                        self.padding, self.dilation, self.groups)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        b, _, _, _ = inputs.size()\n",
    "        res = []\n",
    "        for input in inputs:\n",
    "            input = input.unsqueeze(0)\n",
    "            pooled_inputs = self._avg_pooling(input)\n",
    "            routing_weights = self._routing_fn(pooled_inputs)\n",
    "            kernels = torch.sum(routing_weights[: ,None, None, None, None] * self.weight, 0)\n",
    "            out = self._conv_forward(input, kernels)\n",
    "            res.append(out)\n",
    "        return torch.cat(res, dim=0)\n",
    "    \n",
    "img = torch.Tensor(8, 32, 100,150)\n",
    "print(img.shape)\n",
    "import time\n",
    "start = time.time()\n",
    "print(CondConv2D(in_channels=32, out_channels=32, kernel_size=1)(img).shape)\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c1586bc-e067-4dae-97bb-bab0de631870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total parameters: 50468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/torch/nn/functional.py:1639: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import math\n",
    "\n",
    "from torchvision import transforms\n",
    "import cv2\n",
    "import numpy as np\n",
    "from timm.models.layers import trunc_normal_\n",
    "from model.blocks import CBlock_ln, SwinTransformerBlock\n",
    "from model.CondConv2D import CondConv2D\n",
    "\n",
    "\n",
    "# Short Cut Connection on Final Layer\n",
    "class Local_pred_S(nn.Module):\n",
    "    def __init__(self, in_dim=3, dim=16, number=1):\n",
    "        super(Local_pred_S, self).__init__()\n",
    "        # initial convolution\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_dim, dim, 3, padding=1, groups=1),\n",
    "            nn.Conv2d(dim, dim, 3, padding=1)\n",
    "        )\n",
    "        \n",
    "        self.relu = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n",
    "        block_t = SwinTransformerBlock(dim)  # head number\n",
    "        \n",
    "        blocks1, blocks2 = [block_t for _ in range(number)], [block_t for _ in range(number)]\n",
    "        blocks1.append(CondConv2D(in_channels=dim, out_channels=dim, kernel_size=1))\n",
    "        blocks2.append(CondConv2D(in_channels=dim, out_channels=dim, kernel_size=1))\n",
    "        self.mul_blocks = nn.Sequential(*blocks1)\n",
    "        self.add_blocks = nn.Sequential(*blocks2)\n",
    "\n",
    "        self.mul_end = nn.Sequential(nn.Conv2d(dim, 3, 3, padding = 1), nn.ReLU())\n",
    "        self.add_end = nn.Sequential(nn.Conv2d(dim, 3, 3, padding = 1), nn.Tanh())\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "        elif isinstance(m, nn.Conv2d):\n",
    "            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "            fan_out //= m.groups\n",
    "            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def forward(self, img):\n",
    "        img1 = self.relu(self.conv1(img))\n",
    "        # short cut connection\n",
    "        mul = self.mul_blocks(img1) + img1\n",
    "        add = self.add_blocks(img1) + img1\n",
    "        mul = self.mul_end(mul)\n",
    "        add = self.add_end(add)\n",
    "        return mul, add\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, ffn_expansion_factor, bias):\n",
    "        super(FeedForward, self).__init__()\n",
    "\n",
    "        hidden_features = int(dim*ffn_expansion_factor)\n",
    "\n",
    "        self.project_in = nn.Conv2d(dim, hidden_features*2, kernel_size=1, bias=bias)\n",
    "\n",
    "        self.dwconv = nn.Conv2d(hidden_features*2, hidden_features*2, kernel_size=3, stride=1, padding=1, groups=hidden_features*2, bias=bias)\n",
    "\n",
    "        self.project_out = nn.Conv2d(hidden_features, dim, kernel_size=1, bias=bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.project_in(x)\n",
    "        x1, x2 = self.dwconv(x).chunk(2, dim=1)\n",
    "        x = F.relu(x2)*x1 + F.relu(x1)*x2\n",
    "        x = self.project_out(x)\n",
    "        return x    \n",
    "\n",
    "class Net5(nn.Module):\n",
    "    def __init__(self, in_dim=3):\n",
    "        super(Net5, self).__init__()\n",
    "        #self.local_net = Local_pred()\n",
    "        self.local_net = Local_pred_S(in_dim=in_dim, dim=32)\n",
    "        self.fft_block = FFT_Block()\n",
    "        \n",
    "    def apply_color(self, image, ccm):\n",
    "        shape = image.shape\n",
    "        image = image.view(-1, 3)\n",
    "        image = torch.tensordot(image, ccm, dims=[[-1], [-1]])\n",
    "        image = image.view(shape)\n",
    "        return torch.clamp(image, 1e-8, 1.0)\n",
    "\n",
    "    def forward(self, img_low):\n",
    "        #print(self.with_global)\n",
    "        mul, add = self.local_net(img_low)\n",
    "        img_high = (img_low.mul(mul)).add(add)\n",
    "        \n",
    "        \n",
    "        fft_map = self.fft_block(img_low)\n",
    "        img_high = img_high + fft_map\n",
    "        \n",
    "        return mul, add, img_high\n",
    "        \n",
    "class global_module(nn.Module):\n",
    "    def __init__(self, inplanes, act=nn.LeakyReLU(negative_slope=0.2,inplace=True), bias=False):\n",
    "        super(global_module, self).__init__()\n",
    "\n",
    "        self.conv_mask = nn.Conv2d(inplanes, inplanes, kernel_size=1, bias=bias)\n",
    "        self.linear = nn.Linear(inplanes, 1)\n",
    "\n",
    "    def spatial_pool(self, x):\n",
    "        batch, channel, height, width = x.size()\n",
    "        input_x = x\n",
    "        input_x = input_x.view(batch, channel, height * width)\n",
    "        input_x = input_x.unsqueeze(1)\n",
    "        \n",
    "        context_mask = self.conv_mask(x)\n",
    "        context_mask = context_mask.view(batch, -1, channel)\n",
    "        context_mask = self.linear(context_mask)\n",
    "        context_mask = context_mask.unsqueeze(1)\n",
    "        context = torch.matmul(input_x, context_mask)\n",
    "        context = context.view(batch, channel, 1, 1)\n",
    "\n",
    "        return context\n",
    "\n",
    "    def forward(self, x):\n",
    "        # [N, C, 1, 1]\n",
    "        context = self.spatial_pool(x)\n",
    "        x = x + context\n",
    "        return x\n",
    "    \n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_planes, ratio=2):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "           \n",
    "        self.fc = nn.Sequential(nn.Conv2d(in_planes, in_planes // 16, 1, bias=False),\n",
    "                               nn.ReLU(),\n",
    "                               nn.Conv2d(in_planes // 16, in_planes, 1, bias=False))\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = self.fc(self.avg_pool(x))\n",
    "        max_out = self.fc(self.max_pool(x))\n",
    "        out = avg_out + max_out\n",
    "        return out + x\n",
    "\n",
    "\n",
    "class conv_embedding(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(conv_embedding, self).__init__()\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.GELU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)\n",
    "        return x\n",
    "    \n",
    "class CA(nn.Module):\n",
    "    \"\"\" Channel attention module\"\"\"\n",
    "    def __init__(self, in_dim):\n",
    "        super(CA, self).__init__()\n",
    "        self.chanel_in = in_dim\n",
    "\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "        self.softmax  = nn.Softmax(dim=-1)\n",
    "    def forward(self,x, fft_map=None):\n",
    "        \"\"\"\n",
    "            inputs :\n",
    "                x : input feature maps( B X C X H X W)\n",
    "            returns :\n",
    "                out : attention value + input feature\n",
    "                attention: B X C X C\n",
    "        \"\"\"\n",
    "        m_batchsize, C, height, width = x.size()\n",
    "        proj_query = x.view(m_batchsize, C, -1)\n",
    "        proj_key = x.view(m_batchsize, C, -1).permute(0, 2, 1)\n",
    "        energy = torch.bmm(proj_query, proj_key)\n",
    "        energy_new = torch.max(energy, -1, keepdim=True)[0].expand_as(energy)-energy\n",
    "        attention = self.softmax(energy_new)\n",
    "        proj_value = x.view(m_batchsize, C, -1)\n",
    "\n",
    "        out = torch.bmm(attention, proj_value)\n",
    "        out = out.view(m_batchsize, C, height, width)\n",
    "\n",
    "        out = self.gamma*out + x\n",
    "        return out\n",
    "    \n",
    "    \n",
    "\n",
    "def fft_img(img):\n",
    "    \n",
    "    f = np.fft.fft2(img)\n",
    "    fshift = np.fft.fftshift(f)\n",
    "    rows, cols = img.shape\n",
    "    crow, ccol = int(rows/2), int(cols/2)\n",
    "\n",
    "    fshift[crow-30:crow+30, ccol-30:ccol+30] = 0\n",
    "    ishift = np.fft.ifftshift(fshift)\n",
    "    iimg = np.fft.ifft2(ishift)\n",
    "    iimg = np.abs(iimg) \n",
    "    \n",
    "    return transforms.ToTensor()(iimg)\n",
    "\n",
    "\n",
    "class FFT_Block(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=32, num_heads=4, type='exp'):\n",
    "        super(FFT_Block, self).__init__()\n",
    "        # main blocks\n",
    "        self.conv_large = conv_embedding(in_channels, out_channels)\n",
    "        self.block_t = SwinTransformerBlock(out_channels)\n",
    "        self.generator = CA(in_dim=out_channels)\n",
    "        self.conv_fft = nn.Conv2d(3, 3, kernel_size=3, stride=(1, 1), padding=(1, 1))\n",
    "        self.conv_A = nn.Conv2d(out_channels, 3, kernel_size=3, stride=(1, 1), padding=(1, 1))\n",
    "        self.gm = ChannelAttention(out_channels)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        fft_x = self.conv_fft(x)\n",
    "        b,c,h,w = fft_x.shape\n",
    "        fft_map = torch.zeros(b, 32, h, w)\n",
    "        for i in range(b):\n",
    "            \n",
    "            img = fft_x[i, :, :, :].cpu().detach().numpy().transpose(1,2,0) # b c h w -> h w c\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            fft = fft_img(img*255)/255\n",
    "            fft = torch.cat([fft for i in range(32)], dim=0)\n",
    "            \n",
    "            fft_map[i, :,:,:] = fft\n",
    "        x = self.conv_large(x)\n",
    "        x = self.block_t(x)\n",
    "        x = self.generator(x, fft_map)   \n",
    "        x = self.gm(x)\n",
    "        x = self.conv_A(x)\n",
    "        return x  \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    os.environ['CUDA_VISIBLE_DEVICES']='3'\n",
    "    img = torch.Tensor(1, 3, 400, 600)\n",
    "    net = Net5()\n",
    "    print('total parameters:', sum(param.numel() for param in net.parameters()))\n",
    "    _, _, high = net(img)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7eb64105-03f9-4178-8f93-a17861b8fc16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total parameters: 91154\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import math\n",
    "\n",
    "from timm.models.layers import trunc_normal_\n",
    "from model.blocks import CBlock_ln, SwinTransformerBlock\n",
    "from model.global_net import Global_pred\n",
    "\n",
    "class Local_pred(nn.Module):\n",
    "    def __init__(self, dim=16, number=4, type='ccc'):\n",
    "        super(Local_pred, self).__init__()\n",
    "        # initial convolution\n",
    "        self.conv1 = nn.Conv2d(3, dim, 3, padding=1, groups=1)\n",
    "        self.relu = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n",
    "        # main blocks\n",
    "        block = CBlock_ln(dim)\n",
    "        block_t = SwinTransformerBlock(dim)  # head number\n",
    "        if type =='ccc':  \n",
    "            #blocks1, blocks2 = [block for _ in range(number)], [block for _ in range(number)]\n",
    "            blocks1 = [CBlock_ln(16, drop_path=0.01), CBlock_ln(16, drop_path=0.05), CBlock_ln(16, drop_path=0.1)]\n",
    "            blocks2 = [CBlock_ln(16, drop_path=0.01), CBlock_ln(16, drop_path=0.05), CBlock_ln(16, drop_path=0.1)]\n",
    "        elif type =='ttt':\n",
    "            blocks1, blocks2 = [block_t for _ in range(number)], [block_t for _ in range(number)]\n",
    "        elif type =='cct':\n",
    "            blocks1, blocks2 = [block, block, block_t], [block, block, block_t]\n",
    "        #    block1 = [CBlock_ln(16), nn.Conv2d(16,24,3,1,1)]\n",
    "        self.mul_blocks = nn.Sequential(*blocks1, nn.Conv2d(dim, 3, 3, 1, 1), nn.ReLU())\n",
    "        self.add_blocks = nn.Sequential(*blocks2, nn.Conv2d(dim, 3, 3, 1, 1), nn.Tanh())\n",
    "\n",
    "\n",
    "    def forward(self, img):\n",
    "        img1 = self.relu(self.conv1(img))\n",
    "        mul = self.mul_blocks(img1)\n",
    "        add = self.add_blocks(img1)\n",
    "\n",
    "        return mul, add\n",
    "\n",
    "# Short Cut Connection on Final Layer\n",
    "class Local_pred_S(nn.Module):\n",
    "    def __init__(self, in_dim=3, dim=16, number=4, type='ccc'):\n",
    "        super(Local_pred_S, self).__init__()\n",
    "        # initial convolution\n",
    "        self.conv1 = nn.Conv2d(in_dim, dim, 3, padding=1, groups=1)\n",
    "        self.relu = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n",
    "        # main blocks\n",
    "        block = CBlock_ln(dim)\n",
    "        block_t = SwinTransformerBlock(dim)  # head number\n",
    "        if type =='ccc':\n",
    "            blocks1 = [CBlock_ln(16, drop_path=0.01), CBlock_ln(16, drop_path=0.05), CBlock_ln(16, drop_path=0.1)]\n",
    "            blocks2 = [CBlock_ln(16, drop_path=0.01), CBlock_ln(16, drop_path=0.05), CBlock_ln(16, drop_path=0.1)]\n",
    "        elif type =='ttt':\n",
    "            blocks1, blocks2 = [block_t for _ in range(number)], [block_t for _ in range(number)]\n",
    "        elif type =='cct':\n",
    "            blocks1, blocks2 = [block, block, block_t], [block, block, block_t]\n",
    "        #    block1 = [CBlock_ln(16), nn.Conv2d(16,24,3,1,1)]\n",
    "        self.mul_blocks = nn.Sequential(*blocks1)\n",
    "        self.add_blocks = nn.Sequential(*blocks2)\n",
    "\n",
    "        self.mul_end = nn.Sequential(nn.Conv2d(dim, 3, 3, 1, 1), nn.ReLU())\n",
    "        self.add_end = nn.Sequential(nn.Conv2d(dim, 3, 3, 1, 1), nn.Tanh())\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "        elif isinstance(m, nn.Conv2d):\n",
    "            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "            fan_out //= m.groups\n",
    "            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.zero_()\n",
    "            \n",
    "            \n",
    "\n",
    "    def forward(self, img):\n",
    "        img1 = self.relu(self.conv1(img))\n",
    "        # short cut connection\n",
    "        mul = self.mul_blocks(img1) + img1\n",
    "        add = self.add_blocks(img1) + img1\n",
    "        mul = self.mul_end(mul)\n",
    "        add = self.add_end(add)\n",
    "\n",
    "        return mul, add\n",
    "\n",
    "class IAT(nn.Module):\n",
    "    def __init__(self, in_dim=3, with_global=True, type='lol'):\n",
    "        super(IAT, self).__init__()\n",
    "        #self.local_net = Local_pred()\n",
    "        \n",
    "        self.local_net = Local_pred_S(in_dim=in_dim)\n",
    "\n",
    "        self.with_global = with_global\n",
    "        if self.with_global:\n",
    "            self.global_net = Global_pred(in_channels=in_dim, type=type)\n",
    "\n",
    "    def apply_color(self, image, ccm):\n",
    "        shape = image.shape\n",
    "        image = image.view(-1, 3)\n",
    "        image = torch.tensordot(image, ccm, dims=[[-1], [-1]])\n",
    "        image = image.view(shape)\n",
    "        return torch.clamp(image, 1e-8, 1.0)\n",
    "\n",
    "    def forward(self, img_low):\n",
    "        #print(self.with_global)\n",
    "        mul, add = self.local_net(img_low)\n",
    "        img_high = (img_low.mul(mul)).add(add)\n",
    "\n",
    "        if not self.with_global:\n",
    "            return mul, add, img_high\n",
    "        \n",
    "        else:\n",
    "            gamma, color = self.global_net(img_low)\n",
    "            b = img_high.shape[0]\n",
    "            img_high = img_high.permute(0, 2, 3, 1)  # (B,C,H,W) -- (B,H,W,C)\n",
    "            img_high = torch.stack([self.apply_color(img_high[i,:,:,:], color[i,:,:])**gamma[i,:] for i in range(b)], dim=0)\n",
    "            img_high = img_high.permute(0, 3, 1, 2)  # (B,H,W,C) -- (B,C,H,W)\n",
    "            return mul, add, img_high\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    os.environ['CUDA_VISIBLE_DEVICES']='3'\n",
    "    img = torch.Tensor(1, 3, 400, 600)\n",
    "    net = IAT()\n",
    "    print('total parameters:', sum(param.numel() for param in net.parameters()))\n",
    "    _, _, high = net(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed582b96-4477-4fd4-ae6d-618aff22930e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(608, 608, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(608, 608, 3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "arr = np.ones((608,608))\n",
    "arr = np.expand_dims(arr, axis=2)\n",
    "print(arr.shape)\n",
    "img = np.concatenate((arr,arr,arr), axis=2)\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0a2cbd5-2b92-4c92-a801-3310d1c156db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import math\n",
    "\n",
    "from torchvision import transforms\n",
    "import cv2\n",
    "import numpy as np\n",
    "from timm.models.layers import trunc_normal_\n",
    "from model.blocks import CBlock_ln, SwinTransformerBlock\n",
    "from model.CondConv2D import CondConv2D\n",
    "\n",
    "\n",
    "# Short Cut Connection on Final Layer\n",
    "class Local_pred_S(nn.Module):\n",
    "    def __init__(self, in_dim=3, dim=16, number=1):\n",
    "        super(Local_pred_S, self).__init__()\n",
    "        # initial convolution\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_dim, dim, 3, padding=1, groups=1),\n",
    "            nn.Conv2d(dim, dim, 3, padding=1)\n",
    "        )\n",
    "        \n",
    "        self.relu = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n",
    "        block_t = SwinTransformerBlock(dim)  # head number\n",
    "        \n",
    "        blocks1, blocks2 = [block_t for _ in range(number)], [block_t for _ in range(number)]\n",
    "        blocks1.append(CondConv2D(in_channels=dim, out_channels=dim, kernel_size=1))\n",
    "        blocks2.append(CondConv2D(in_channels=dim, out_channels=dim, kernel_size=1))\n",
    "        self.mul_blocks = nn.Sequential(*blocks1)\n",
    "        self.add_blocks = nn.Sequential(*blocks2)\n",
    "\n",
    "        self.mul_end = nn.Sequential(nn.Conv2d(dim, 3, 3, padding = 1), nn.ReLU())\n",
    "        self.add_end = nn.Sequential(nn.Conv2d(dim, 3, 3, padding = 1), nn.Tanh())\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "        elif isinstance(m, nn.Conv2d):\n",
    "            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "            fan_out //= m.groups\n",
    "            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def forward(self, img):\n",
    "        img1 = self.relu(self.conv1(img))\n",
    "        # short cut connection\n",
    "        mul = self.mul_blocks(img1) + img1\n",
    "        add = self.add_blocks(img1) + img1\n",
    "        mul = self.mul_end(mul)\n",
    "        add = self.add_end(add)\n",
    "        return mul, add\n",
    " \n",
    "\n",
    "class Net6(nn.Module):\n",
    "    def __init__(self, in_dim=3):\n",
    "        super(Net6, self).__init__()\n",
    "        #self.local_net = Local_pred()\n",
    "        self.local_net = Local_pred_S(in_dim=in_dim, dim=32)\n",
    "        \n",
    "    def apply_color(self, image, ccm):\n",
    "        shape = image.shape\n",
    "        image = image.view(-1, 3)\n",
    "        image = torch.tensordot(image, ccm, dims=[[-1], [-1]])\n",
    "        image = image.view(shape)\n",
    "        return torch.clamp(image, 1e-8, 1.0)\n",
    "\n",
    "    def forward(self, img_low):\n",
    "        #print(self.with_global)\n",
    "        mul, add = self.local_net(img_low)\n",
    "        img_high = (img_low.mul(mul)).add(add)\n",
    "        \n",
    "        \n",
    "#         fft_map = self.fft_block(img_low)\n",
    "#         img_high = img_high + fft_map\n",
    "        \n",
    "        return mul, add, img_high\n",
    "        \n",
    "class global_module(nn.Module):\n",
    "    def __init__(self, inplanes, act=nn.LeakyReLU(negative_slope=0.2,inplace=True), bias=False):\n",
    "        super(global_module, self).__init__()\n",
    "\n",
    "        self.conv_mask = nn.Conv2d(inplanes, inplanes, kernel_size=1, bias=bias)\n",
    "        self.linear = nn.Linear(inplanes, 1)\n",
    "\n",
    "    def spatial_pool(self, x):\n",
    "        batch, channel, height, width = x.size()\n",
    "        input_x = x\n",
    "        input_x = input_x.view(batch, channel, height * width)\n",
    "        input_x = input_x.unsqueeze(1)\n",
    "        \n",
    "        context_mask = self.conv_mask(x)\n",
    "        context_mask = context_mask.view(batch, -1, channel)\n",
    "        context_mask = self.linear(context_mask)\n",
    "        context_mask = context_mask.unsqueeze(1)\n",
    "        context = torch.matmul(input_x, context_mask)\n",
    "        context = context.view(batch, channel, 1, 1)\n",
    "\n",
    "        return context\n",
    "\n",
    "    def forward(self, x):\n",
    "        # [N, C, 1, 1]\n",
    "        context = self.spatial_pool(x)\n",
    "        x = x + context\n",
    "        return x\n",
    "    \n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_planes, ratio=2):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "           \n",
    "        self.fc = nn.Sequential(nn.Conv2d(in_planes, in_planes // 16, 1, bias=False),\n",
    "                               nn.ReLU(),\n",
    "                               nn.Conv2d(in_planes // 16, in_planes, 1, bias=False))\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = self.fc(self.avg_pool(x))\n",
    "        max_out = self.fc(self.max_pool(x))\n",
    "        out = avg_out + max_out\n",
    "        return out + x\n",
    "\n",
    "\n",
    "class conv_embedding(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(conv_embedding, self).__init__()\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.GELU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)\n",
    "        return x\n",
    "    \n",
    "class CA(nn.Module):\n",
    "    \"\"\" Channel attention module\"\"\"\n",
    "    def __init__(self, in_dim):\n",
    "        super(CA, self).__init__()\n",
    "        self.chanel_in = in_dim\n",
    "\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "        self.softmax  = nn.Softmax(dim=-1)\n",
    "    def forward(self,x, fft_map=None):\n",
    "        \"\"\"\n",
    "            inputs :\n",
    "                x : input feature maps( B X C X H X W)\n",
    "            returns :\n",
    "                out : attention value + input feature\n",
    "                attention: B X C X C\n",
    "        \"\"\"\n",
    "        m_batchsize, C, height, width = x.size()\n",
    "        proj_query = x.view(m_batchsize, C, -1)\n",
    "        proj_key = x.view(m_batchsize, C, -1).permute(0, 2, 1)\n",
    "        energy = torch.bmm(proj_query, proj_key)\n",
    "        energy_new = torch.max(energy, -1, keepdim=True)[0].expand_as(energy)-energy\n",
    "        attention = self.softmax(energy_new)\n",
    "        proj_value = x.view(m_batchsize, C, -1)\n",
    "\n",
    "        out = torch.bmm(attention, proj_value)\n",
    "        out = out.view(m_batchsize, C, height, width)\n",
    "\n",
    "        out = self.gamma*out + x\n",
    "        return out\n",
    "    \n",
    "    \n",
    "\n",
    "def fft_img(img):\n",
    "    \n",
    "    f = np.fft.fft2(img)\n",
    "    fshift = np.fft.fftshift(f)\n",
    "    rows, cols = img.shape\n",
    "    crow, ccol = int(rows/2), int(cols/2)\n",
    "\n",
    "    fshift[crow-30:crow+30, ccol-30:ccol+30] = 0\n",
    "    ishift = np.fft.ifftshift(fshift)\n",
    "    iimg = np.fft.ifft2(ishift)\n",
    "    iimg = np.abs(iimg) \n",
    "    \n",
    "    return transforms.ToTensor()(iimg)\n",
    "\n",
    "\n",
    "class FFT_Block(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=32, num_heads=4, type='exp'):\n",
    "        super(FFT_Block, self).__init__()\n",
    "        # main blocks\n",
    "        self.conv_large = conv_embedding(in_channels, out_channels)\n",
    "        self.block_t = SwinTransformerBlock(out_channels)\n",
    "        self.generator = CA(in_dim=out_channels)\n",
    "        self.conv_fft = nn.Conv2d(3, 3, kernel_size=3, stride=(1, 1), padding=(1, 1))\n",
    "        self.conv_A = nn.Conv2d(out_channels, 3, kernel_size=3, stride=(1, 1), padding=(1, 1))\n",
    "        self.gm = ChannelAttention(out_channels)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        fft_x = self.conv_fft(x)\n",
    "        b,c,h,w = fft_x.shape\n",
    "        fft_map = torch.zeros(b, 32, h, w)\n",
    "        for i in range(b):\n",
    "            \n",
    "            img = fft_x[i, :, :, :].cpu().detach().numpy().transpose(1,2,0) # b c h w -> h w c\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            fft = fft_img(img*255)/255\n",
    "            fft = torch.cat([fft for i in range(32)], dim=0)\n",
    "            \n",
    "            fft_map[i, :,:,:] = fft\n",
    "        x = self.conv_large(x)\n",
    "        x = self.block_t(x)\n",
    "        x = self.generator(x, fft_map)   \n",
    "        x = self.gm(x)\n",
    "        x = self.conv_A(x)\n",
    "        return x  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1fdee33-07a9-42ae-ab66-4c4ecc54a7fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total parameters: 33356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/torch/nn/functional.py:1639: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    }
   ],
   "source": [
    "img = torch.Tensor(1, 3, 400, 600)\n",
    "net = Net6()\n",
    "print('total parameters:', sum(param.numel() for param in net.parameters()))\n",
    "_, _, high = net(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b64c4294-f064-4f4a-806b-fca87966e2e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "85\n",
      "96\n"
     ]
    }
   ],
   "source": [
    "# 90 10 27 48\n",
    "images = ['low00746.png', 'low00737.png', 'low00775.png', 'low00762.png', 'low00767.png', 'low00742.png', 'low00709.png', 'low00756.png', 'low00705.png', 'low00764.png', 'low00783.png', 'low00771.png', 'low00699.png', 'low00738.png', 'low00706.png', 'low00741.png', 'low00739.png', 'low00704.png', 'low00752.png', 'low00744.png', 'low00748.png', 'low00784.png', 'low00779.png', 'low00789.png', 'low00725.png', 'low00768.png', 'low00736.png', 'low00732.png', 'low00751.png', 'low00785.png', 'low00788.png', 'low00723.png', 'low00754.png', 'low00734.png', 'low00729.png', 'low00728.png', 'low00713.png', 'low00693.png', 'low00692.png', 'low00718.png', 'low00770.png', 'low00782.png', 'low00710.png', 'low00750.png', 'low00703.png', 'low00691.png', 'low00720.png', 'low00730.png', 'low00719.png', 'low00773.png', 'low00721.png', 'low00716.png', 'low00701.png', 'low00715.png', 'low00740.png', 'low00776.png', 'low00711.png', 'low00714.png', 'low00758.png', 'low00777.png', 'low00787.png', 'low00763.png', 'low00697.png', 'low00781.png', 'low00724.png', 'low00733.png', 'low00786.png', 'low00695.png', 'low00717.png', 'low00698.png', 'low00753.png', 'low00766.png', 'low00743.png', 'low00735.png', 'low00727.png', 'low00774.png', 'low00759.png', 'low00707.png', 'low00749.png', 'low00700.png', 'low00696.png', 'low00765.png', 'low00760.png', 'low00702.png', 'low00690.png', 'low00778.png', 'low00755.png', 'low00757.png', 'low00708.png', 'low00731.png', 'low00722.png', 'low00694.png', 'low00747.png', 'low00745.png', 'low00769.png', 'low00772.png', 'low00780.png', 'low00726.png', 'low00712.png', 'low00761.png']\n",
    "print(images.index('low00748.png'))\n",
    "print(images.index('low00778.png'))\n",
    "print(images.index('low00780.png'))\n",
    "\n",
    "# print(images[10])\n",
    "# print(images[27])\n",
    "# print(images[48])\n",
    "# print(images[90])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe43d0a-2c7b-4511-8f61-d323379017c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
